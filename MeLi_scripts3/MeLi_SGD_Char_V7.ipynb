{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import balanced_accuracy_score as bas\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/store/tveiga/miniconda3/envs/py/lib/python3.7/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# data = pd.read_csv('../data-simplified-1-reduced-wordbal-800.csv')\n",
    "data = pd.read_csv('../data-reduced-800-v3-shuffled.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcode = pd.read_csv('../data-simplified-1-catcode.csv', header = None, names = ['category'])['category'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "      <th>priorities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>473424</th>\n",
       "      <td>Kit 04 Jogo De Lençol De Berço Em Malha 3 Pcs 100% Algodão</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "      <td>6245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7519083</th>\n",
       "      <td>Bomba Submersa 450 W Agua Suja Turva Limpa Bsv 450 Vonder</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1360</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19488607</th>\n",
       "      <td>Nadador Tiburon Ys1378-5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1155</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16895633</th>\n",
       "      <td>Máscara Angry Birds 6un Imbatível</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1102</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10369454</th>\n",
       "      <td>Aparador Fruteira Madeira De Demolição 1 Gaveta Peroba Rosa</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1288</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                title  \\\n",
       "473424     Kit 04 Jogo De Lençol De Berço Em Malha 3 Pcs 100% Algodão   \n",
       "7519083     Bomba Submersa 450 W Agua Suja Turva Limpa Bsv 450 Vonder   \n",
       "19488607                                     Nadador Tiburon Ys1378-5   \n",
       "16895633                            Máscara Angry Birds 6un Imbatível   \n",
       "10369454  Aparador Fruteira Madeira De Demolição 1 Gaveta Peroba Rosa   \n",
       "\n",
       "          label_quality  language  category  priorities  \n",
       "473424                0         1       114        6245  \n",
       "7519083               1         1      1360           4  \n",
       "19488607              1         0      1155          54  \n",
       "16895633              0         1      1102         486  \n",
       "10369454              0         1      1288        1075  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(curr):\n",
    "    # remove accent\n",
    "    curr = curr.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    # to lower case\n",
    "    curr = curr.str.lower()\n",
    "    # remove not alphanumerics or . ,\n",
    "    curr = curr.str.replace('[^a-zA-Z0-9.,]', ' ')\n",
    "    \n",
    "    # let , and . be the same char\n",
    "    curr = curr.str.replace('[.]', ',')\n",
    "    \n",
    "    # remove . , not between numbers\n",
    "    curr = curr.str.replace('(?<=[0-9])[,]+(?=[0-9])', '.')\n",
    "    curr = curr.str.replace('[,]', ' ')\n",
    "    \n",
    "    # set all digits to 0\n",
    "    curr = curr.str.replace('[0-9]', '0')\n",
    "    \n",
    "    # separate ' <digits><letters ' like in 22g or 12ms\n",
    "    # curr = curr.str.replace('(^| )([0-9]+)([a-zA-Z]+)($| )', r'\\1\\2 \\3\\4')\n",
    "    \n",
    "    # remove some Pt plurals\n",
    "    curr = curr.str.replace('\\\\b([a-zA-Z]+[aeiouwy])(s)\\\\b', r'\\1')\n",
    "    \n",
    "    # remove 4 consec (same) letters to just one\n",
    "    curr = curr.str.replace(r'([a-zA-Z])\\1{3,}', r'\\1') # 3 is four? -> three of \\1 after first \\1...\n",
    "    \n",
    "    # separate 4 or more consecutive (different or not) letters\n",
    "    curr = curr.str.replace(r'([a-zA-Z]{4,})', r' \\1 ')\n",
    "    \n",
    "    # Other ideas: \n",
    "    \n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /store/tveiga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = lambda w : unicodedata.normalize('NFKD', w).encode('ASCII', 'ignore').decode('ASCII')\n",
    "all_stopw = set()\n",
    "for corpus in ['english', 'portuguese', 'spanish']:\n",
    "    all_stopw.update(set(map(norm, stopwords.words(corpus))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = int(len(data) * 0.8) # Split Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([data[['title']], test[['title']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 s, sys: 324 ms, total: 29.9 s\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_full = full.title\n",
    "X_full = normalize(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 s, sys: 704 ms, total: 17.3 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "wordfreq = X_full.str.split(expand=True).stack().value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102942\n",
      "CPU times: user 34.7 ms, sys: 0 ns, total: 34.7 ms\n",
      "Wall time: 34.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "uniquewords = {w for w, f in wordfreq.items() if f == 1}\n",
    "print(len(uniquewords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3906\n"
     ]
    }
   ],
   "source": [
    "testwordfreq = X_full[len(data):].str.split(expand=True).stack().value_counts().to_dict()\n",
    "testuniquewords = {w for w, f in testwordfreq.items() if wordfreq[w] == 1}\n",
    "print(len(testuniquewords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unkn = lambda x : 'U0' if '0' in x else 'UA' # ('UFT' if x in ftwords else 'UA')\n",
    "xjoin = lambda s : ' '.join([w if w not in uniquewords else unkn(w) for w in s ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.2 s, sys: 72 ms, total: 4.27 s\n",
      "Wall time: 4.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_full = X_full.str.split().apply(xjoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.28 s, sys: 52 ms, total: 4.33 s\n",
      "Wall time: 4.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xremo = lambda s : ' '.join([w for w in s if w not in all_stopw])\n",
    "X_full = X_full.str.split().apply(xremo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.48 s, sys: 31.9 ms, total: 2.51 s\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# go full crazy!\n",
    "X_full = X_full.str.split().str.join('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_full = '1' + X_full + '2' #actaully don't need it when joining everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_1gram = X_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1622778, 275175) 120623543\n",
      "CPU times: user 1min 1s, sys: 1.56 s, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "covec_1gram = CountVectorizer(binary = True, min_df= 2, analyzer = 'char_wb', max_df = .95,\n",
    "                             ngram_range=(3,4),)\n",
    "X_covec_1gram = covec_1gram.fit_transform(X_full)\n",
    "print(X_covec_1gram.shape, X_covec_1gram.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docfreq1 = np.array(X_covec_1gram.sum(axis = 0)).flatten() / X_covec_1gram.shape[0]\n",
    "inv_vocab1 = {v : k for k,v in covec_1gram.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cab', '0mm', 'gin', 'elo', 'cap', 'ras', '00g', 'der', 'inh',\n",
       "       'lado', 'ente', 'abl', 'til', 'cion', 'nda', 'ral', 'las', 'men',\n",
       "       'rom', 'erm', 'uni', 'adi', 'ste', 'inal', 'sti', 'rio', 'lac',\n",
       "       'qui', 'l00', 'rin', 'tad', 'ula', 'ase', 'ole', 'tac', 'ato',\n",
       "       'ric', '0x00', 'cal', 'ari', 'rol', 'rig', 'can', '0cm', 'cao',\n",
       "       'rte', 'rra', 'ram', 'oco', 'dora', 'ini', 'eco', '00x0', 'ino',\n",
       "       'ont', 'ame', 'ata', 'ano', 'res', '0.00', '.00', 'lar', 'ver',\n",
       "       't00', 'ave', 'ura', 'lla', 'ble', 'art', 'ante', 'cio', 'lin',\n",
       "       'eto', 'ast', 'orta', 'nti', '00v', 'ill', 'dad', 'ria', 'o000',\n",
       "       'ana', 'bra', 'cha', 'cam', 'and', 'nto', 'tec', 'oto', 'etr',\n",
       "       'ale', 'r00', 'cad', ' kit', 'igi', 'que', 'ote', 'man', 'int',\n",
       "       ' ki', 'ero', '00x', 'lan', 'arr', 'eri', ' co', 'ela', 'tri',\n",
       "       'mar', 'oma', 'col', 'lad', '00p', '000m', 'ret', 'pla', 'eira',\n",
       "       'rac', 'rat', 'ali', 'ete', 'eca', 'one', 'ola', 'apa', 'par',\n",
       "       'oca', 'gra', 'min', 'ida', 'per', 'ala', 'com', 'ona', 'anc',\n",
       "       '0x0', 'ita', 'tic', 'pre', 'sta', 'e00', 'con', 'port', 'are',\n",
       "       'nta', 'ate', 'rad', 'a000', 'ele', 'ara', 'ore', 'tal', 'est',\n",
       "       'let', 'ati', 'tra', 'rta', 'ira', 'kit', 'aco', 'ori', 'cor',\n",
       "       'pro', 'ada', 'car', 'por', ' ca', 'eta', 'ran', 'nal', 'eir',\n",
       "       'ion', 'ama', 'era', 'tro', 'x00', 'ora', '00c', 'ico', 'ade',\n",
       "       'tor', 'aca', 'o00', '00m', '0.0', 'ant', 'ort', 'ica', '000 ',\n",
       "       'ter', 'ent', 'ina', 'nte', 'a00', 'ador', '00 ', 'dor', 'ado',\n",
       "       '0000', '000'], dtype='<U4')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vectorize(inv_vocab1.get)(np.argsort(docfreq1)[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09971851, 0.09978383, 0.10190303, 0.1046206 , 0.12050015,\n",
       "       0.13252645, 0.14365797, 0.19747556, 0.21753622, 0.3871084 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(docfreq1)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1622778, 275175) 120623543\n"
     ]
    }
   ],
   "source": [
    "X_train_counts = X_covec_1gram\n",
    "print(X_train_counts.shape, X_train_counts.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbc(x):\n",
    "    # sparse binary correlation; x : sparse\n",
    "    # can't correlate zero columns\n",
    "    cx = sparse.triu(x.T*x, k = 1, format='coo')\n",
    "    # print(cx.todense())\n",
    "    card = np.array(x.sum(axis = 0)).flatten()\n",
    "    # print(card)\n",
    "    cx.data = cx.data / (card[cx.row] + card[cx.col] - cx.data)\n",
    "    # print(cx.todense())\n",
    "    return np.array((cx == 1).sum(axis = 0) > 0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014910511492686473\n",
      "CPU times: user 2min 27s, sys: 16.4 s, total: 2min 43s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rem = sbc(X_train_counts)\n",
    "print(rem.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1622778, 271072) 120283593\n"
     ]
    }
   ],
   "source": [
    "X_train_counts = X_train_counts[:, ~rem]\n",
    "print(X_train_counts.shape, X_train_counts.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_go = X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1622778, 271072)\n",
      "CPU times: user 1.15 s, sys: 300 ms, total: 1.45 s\n",
      "Wall time: 672 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(norm='l2', use_idf=False, smooth_idf=True, sublinear_tf=False)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_go = X_train_tfidf\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1100658, 271072), (275165, 271072))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp2 = len(data)\n",
    "X_train, y_train = X_go[:sp], data.category.values[:sp]\n",
    "X_test, y_test = X_go[sp:sp2], data.category.values[sp:sp2]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weights = (1 / pd.Series(y_train).value_counts()).to_dict() # switching 1 to len(y) seems to make diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = np.vectorize(class_weights.get)(y_train) # * rel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = data.label_quality.values[sp:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 46min 3s, sys: 6min 51s, total: 1h 52min 55s\n",
      "Wall time: 24min 31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/store/tveiga/miniconda3/envs/py/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = sp // 1\n",
    "clf_sgd = SGDClassifier(loss = 'modified_huber', #n_iter = 12,\n",
    "                        max_iter=20, tol=1e-5, # try 1e-6 !!\n",
    "                        alpha = 0.065e-8,\n",
    "#                     early_stopping=True, validation_fraction = .2, n_iter_no_change = 5,\n",
    "                    shuffle = False, n_jobs=4).fit(X_train[:n], y_train[:n],\n",
    "                                                   sample_weight=sample_weight[:n],\n",
    "                                                  )\n",
    "clf = clf_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: 0.8576398827488646\n",
      "Rel: 0.9016474912129034\n",
      "CPU times: user 20.2 s, sys: 2.44 s, total: 22.7 s\n",
      "Wall time: 21.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/store/tveiga/miniconda3/envs/py/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prediction_val = clf.predict(X_test)\n",
    "print('Val:', bas(y_test, prediction_val))\n",
    "\n",
    "rel = data.label_quality.values[sp:]\n",
    "print('Rel:', bas(y_test[rel == 0], prediction_val[rel == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rel:', bas(y_test[rel == 0], prediction_val[rel == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 s, sys: 2.45 s, total: 24 s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_proba = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_proba = pd.DataFrame(val_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 30s, sys: 812 ms, total: 2min 31s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%time val_proba.to_csv('../ensemb3/val_sgd_char-v7.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 58min 52s, sys: 6min 54s, total: 2h 5min 46s\n",
      "Wall time: 27min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_data = data.category\n",
    "X_data = X_go[:sp2]\n",
    "class_weights_data = (1 / pd.Series(y_data).value_counts()).to_dict()\n",
    "sample_weight_data = np.vectorize(class_weights_data.get)(y_data)\n",
    "# rel_data =  1 + (1 - data.label_quality.values) * (relfactor - 1)\n",
    "clf.fit(X_data, y_data, sample_weight=sample_weight_data ) # warm start ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proba = clf.predict_proba(X_go[sp2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proba = pd.DataFrame(test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 16s, sys: 708 ms, total: 2min 17s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%time test_proba.to_csv('../ensemb3/test_sgd_char-v7.csv', index = False, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
